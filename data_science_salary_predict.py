# -*- coding: utf-8 -*-
"""Data_Science_salary_predict.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ebNC5K5Pb_6HyFEQte5nezR1if-wxRzS

Project Name: **Data Science Job salaries**

Prepared On: **Feb 11, 2025**
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor
from lightgbm import LGBMRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

"""### About The Datasets Columns:

**work_year:** The year the salary was paid.

**experience_level:** The experience level of the employee (EN: Entry-level, MI: Mid-level, SE: Senior-level, EX: Executive-level).

**employment_type:** The type of employment (PT: Part-time, FT: Full-time, CT: Contract, FL: Freelance).

**job_title:** The role or job title of the employee during the year.

**salary:** The total gross salary amount paid.

**salary_currency:** The currency of the salary paid (ISO 4217 currency code).

**salary_in_usd:** The salary converted to USD using the average exchange rate for the year.

**employee_residence:** The employee's primary country of residence (ISO 3166 country code).

**remote_ratio:** The percentage of work done remotely (0: None, 50: Partial, 100: Fully remote).

**company_location:** The country of the employer's main office (ISO 3166 country code).

**company_size:** The size of the company (S: Small <50, M: Medium 50-250, L: Large >250 employees).

### Load the Datasets:

*   I have three different datasets df1, df2 and df3
*   df2 having one unneccessary columns, So firstly drop that
*   Then, Concat the three datasets
"""

df1 = pd.read_csv('/content/Data Science Jobs Salaries.csv')
print(df1.shape)
df1.head()

df2 = pd.read_csv('/content/ds_salaries.csv')
print(df2.shape)
df2.head()

df3 = pd.read_csv('/content/ds_salaries1.csv')
print(df3.shape)
df3.head()

"""### Preprocessing"""

# Remove the Unnneccary columns
df2 = df2.drop(columns=['Unnamed: 0'], errors='ignore')

df = pd.concat([df1,df2,df3], ignore_index=True)
print(df.shape)
df.head()

"""### Inspect the Dataset"""

print(df.info())
print(df.describe())
print(df.isnull().sum())  # Check missing values

# Convert categorical 'work_year' to numeric
if df['work_year'].dtype == 'object':
  df['work_year'] = pd.to_numeric(df['work_year'], errors='coerce')

df.info()

df.isnull().sum()

df['work_year'] = df['work_year'].fillna(df['work_year'].mode()[0])

df.duplicated().sum()

# Since we have duplicated values the we remove this
df = df.drop_duplicates(keep='first')
df.duplicated().sum()

df.shape

"""### EDA"""

# Visualize salary distribution
plt.figure(figsize=(10,5))
sns.histplot(df['salary_in_usd'], bins=50, kde=True)
plt.title('Salary Distribution')
plt.show()

# Visualize salary by experience level
plt.figure(figsize=(8, 6))
sns.boxplot(x='experience_level', y='salary_in_usd', data=df)
plt.xlabel('Experience level')
plt.ylabel('salary')
plt.title('Experience Vs Salary Distribution')

# From the boxplot: Their is a big outlier value, so first we remove it because it is poorly effect our model performance
Q1 = df['salary_in_usd'].quantile(0.25)  # 25th percentile
Q3 = df['salary_in_usd'].quantile(0.75)  # 75th percentile
IQR = Q3 - Q1
upper_bound = Q3 + 1.5 * IQR  # Anything above this is an outlier

df = df[df['salary_in_usd'] <= upper_bound]

# let see it again

plt.figure(figsize=(8, 6))
sns.boxplot(x='experience_level', y='salary_in_usd', data=df)
plt.xlabel('Experience level')
plt.ylabel('salary')
plt.title('Experience Vs Salary Distribution')

# Now it looking good ðŸ˜Š

"""`Convert categorical features into numerical using Label Encoding`"""

from sklearn.preprocessing import LabelEncoder

label_cols = ['experience_level', 'employment_type', 'job_title', 'salary_currency',
              'employee_residence', 'company_location', 'company_size']

for col in label_cols:
    le = LabelEncoder()
    df[col] = le.fit_transform(df[col])

df.sample()

# Correlational heatmap
plt.figure(figsize=(10, 6))
sns.heatmap(df.corr(), annot=True, cmap='coolwarm', linewidths=0.5)
plt.title('Feature Correlational Heatmap')
plt.show()

df.sample(5)

# Separate features and target
X = df.drop(columns=['salary_in_usd', 'salary'])  # Drop 'salary' and 'salary_in_usd'
y = df['salary_in_usd']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

X_train.shape, X_test.shape, y_train.shape, y_test.shape

X_train.head()

# Standardize the features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

"""### Model Building"""

# Define a function to evaluate models
def evaluate_model(y_true, y_pred, model_name):
    mae = mean_absolute_error(y_true, y_pred)
    mse = mean_squared_error(y_true, y_pred)
    rmse = np.sqrt(mse)
    r2 = r2_score(y_true, y_pred)

    print(f"Model: {model_name}")
    print(f"MAE: {mae:.2f}")
    print(f"MSE: {mse:.2f}")
    print(f"RMSE: {rmse:.2f}")
    print(f"RÂ² Score: {r2:.2f}")
    print("-" * 50)

# Define models
models = {
    "Linear Regression": LinearRegression(),
    "Decision Tree": DecisionTreeRegressor(random_state=42),
    "Random Forest": RandomForestRegressor(random_state=42),
    "XGBoost": XGBRegressor(random_state=42)
}

import joblib

# Train and evaluate models
best_model = None
best_r2 = -float('inf')  # Initialize with the lowest possible value

for name, model in models.items():
    model.fit(X_train_scaled, y_train)  # Train the model
    y_pred = model.predict(X_test_scaled)  # Make predictions
    r2 = r2_score(y_test, y_pred)  # Calculate RÂ² score

    # Check if this model is the best
    if r2 > best_r2:
        best_r2 = r2
        best_model = model
        best_model_name = name

    # Print evaluation metrics
    evaluate_model(y_test, y_pred, name)

# Save the best model
joblib.dump(best_model, 'best_salary_predictor.pkl')
print(f"\nBest Model: {best_model_name} with RÂ² Score: {best_r2:.2f}")
print("Model saved as 'best_salary_predictor.pkl'")

"""## Load the Model and Predict Salary from User Input"""

loaded_model = joblib.load('best_salary_predictor.pkl')

def predict_user_salary(user_input):
    # Convert the user input into a DataFrame
    user_df = pd.DataFrame([user_input])

    # Ensure the DataFrame has the same columns as the training data
    expected_columns = ['work_year', 'experience_level', 'employment_type', 'job_title',
                        'salary_currency', 'employee_residence', 'remote_ratio',
                        'company_location', 'company_size']  # Exclude 'salary'
    user_df = user_df.reindex(columns=expected_columns)

    # Preprocess the input (same as training data preprocessing)
    label_cols = ['experience_level', 'employment_type', 'job_title', 'salary_currency',
                  'employee_residence', 'company_location', 'company_size']

    for col in label_cols:
        le = LabelEncoder()
        user_df[col] = le.fit_transform(user_df[col])

    # Standardize the input
    user_scaled = scaler.transform(user_df)

    # Predict salary
    predicted_salary = loaded_model.predict(user_scaled)
    return predicted_salary[0]

# Example user input
user_input = {
    'work_year': 2023,
    'experience_level': 'SE',  # Senior-level
    'employment_type': 'FT',   # Full-time
    'job_title': 'Data Scientist',
    'salary_currency': 'USD',
    'employee_residence': 'US',
    'remote_ratio': 100,       # Fully remote
    'company_location': 'US',
    'company_size': 'L'        # Large company
}

# Predict salary
predicted_salary = predict_user_salary(user_input)
print(f"Predicted Salary: ${predicted_salary:.2f}")